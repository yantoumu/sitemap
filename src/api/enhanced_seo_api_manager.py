"""
å¢å¼ºç‰ˆSEO APIç®¡ç†å™¨
æ”¯æŒå¢é‡ä¿å­˜ã€å®¹é”™å¤„ç†å’Œé•¿æ—¶é—´è¿è¡Œä¼˜åŒ–
"""

import asyncio
import aiohttp
import time
from typing import Dict, List, Optional, Any, Set, Callable
from datetime import datetime
import logging

from ..utils.log_security import LogSecurity
from ..utils.incremental_saver import IncrementalSaver
from ..utils.fault_tolerant_processor import FaultTolerantProcessor, BatchResult
from .seo_api_manager import SEOAPIManager


class EnhancedSEOAPIManager(SEOAPIManager):
    """å¢å¼ºç‰ˆSEO APIç®¡ç†å™¨"""
    
    def __init__(self, api_urls: List[str], interval: float = 1.0,
                 batch_size: int = 5, timeout: int = 30,
                 enable_incremental_save: bool = True,
                 enable_fault_tolerance: bool = True,
                 save_interval: int = 1000,
                 git_commit_interval: int = 5000,
                 max_runtime_hours: float = 5.5):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆSEO APIç®¡ç†å™¨
        
        Args:
            api_urls: APIç«¯ç‚¹URLåˆ—è¡¨
            interval: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            enable_incremental_save: æ˜¯å¦å¯ç”¨å¢é‡ä¿å­˜
            enable_fault_tolerance: æ˜¯å¦å¯ç”¨å®¹é”™å¤„ç†
            save_interval: æœ¬åœ°ä¿å­˜é—´éš”ï¼ˆå…³é”®è¯æ•°é‡ï¼‰
            git_commit_interval: Gitæäº¤é—´éš”ï¼ˆå…³é”®è¯æ•°é‡ï¼‰
            max_runtime_hours: æœ€å¤§è¿è¡Œæ—¶é—´ï¼ˆå°æ—¶ï¼‰
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(api_urls, interval, batch_size, timeout)
        
        # å¢é‡ä¿å­˜å™¨
        self.incremental_saver = None
        if enable_incremental_save:
            self.incremental_saver = IncrementalSaver(
                save_interval=save_interval,
                git_commit_interval=git_commit_interval,
                max_runtime_hours=max_runtime_hours
            )
        
        # å®¹é”™å¤„ç†å™¨
        self.fault_processor = None
        if enable_fault_tolerance:
            self.fault_processor = FaultTolerantProcessor(
                max_retries=2,  # æœ€å¤šé‡è¯•2æ¬¡
                retry_delay_base=5.0,  # åŸºç¡€å»¶è¿Ÿ5ç§’
                failure_threshold=0.3,  # 30%å¤±è´¥ç‡é˜ˆå€¼
                circuit_breaker_threshold=5  # è¿ç»­5æ¬¡å¤±è´¥è§¦å‘ç†”æ–­
            )
        
        # å¤„ç†çŠ¶æ€
        self.total_processed_keywords = 0
        self.is_emergency_mode = False
        
        # æ›´æ–°æ—¥å¿—ä¿¡æ¯
        features = []
        if enable_incremental_save:
            features.append("å¢é‡ä¿å­˜")
        if enable_fault_tolerance:
            features.append("å®¹é”™å¤„ç†")
        
        feature_str = f", åŠŸèƒ½: {', '.join(features)}" if features else ""
        
        self.logger.info(f"ğŸš€ å¢å¼ºç‰ˆSEO APIç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ{feature_str}")
    
    async def query_keywords_with_resilience(self,
                                           keywords: List[str],
                                           storage_callback: Optional[Callable] = None,
                                           submission_callback: Optional[Callable] = None) -> Dict[str, Dict]:
        """
        å¸¦å¼¹æ€§å¤„ç†çš„å…³é”®è¯æŸ¥è¯¢
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            storage_callback: å­˜å‚¨å›è°ƒå‡½æ•°
            submission_callback: æäº¤å›è°ƒå‡½æ•°
            
        Returns:
            Dict[str, Dict]: æŸ¥è¯¢ç»“æœ
        """
        self.logger.info(f"ğŸ¯ å¼€å§‹å¼¹æ€§å…³é”®è¯æŸ¥è¯¢: {len(keywords)} ä¸ªå…³é”®è¯")
        
        if self.incremental_saver:
            self.logger.info(f"â±ï¸ è¿è¡Œæ—¶é™åˆ¶: {self.incremental_saver.max_runtime_seconds/3600:.1f} å°æ—¶")
        
        results = {}
        storage_buffer = []
        submission_buffer = []
        
        # åˆ†æ‰¹å¤„ç†
        total_batches = (len(keywords) + self.batch_size - 1) // self.batch_size
        
        for i in range(0, len(keywords), self.batch_size):
            batch_num = i // self.batch_size + 1
            batch = keywords[i:i + self.batch_size]
            
            # æ£€æŸ¥æ˜¯å¦æ¥è¿‘è¶…æ—¶
            if self.incremental_saver and self.incremental_saver.is_approaching_timeout():
                self.logger.warning(f"ğŸš¨ æ¥è¿‘è¶…æ—¶é™åˆ¶ï¼Œè§¦å‘ç´§æ€¥ä¿å­˜")
                await self._emergency_save(storage_buffer, submission_buffer, 
                                         storage_callback, submission_callback)
                self.is_emergency_mode = True
                break
            
            # è·å–ç«¯ç‚¹
            endpoint_index = self._get_next_endpoint()
            
            # å¤„ç†æ‰¹æ¬¡
            if self.fault_processor:
                # ä½¿ç”¨å®¹é”™å¤„ç†
                batch_result = await self.fault_processor.process_batch_with_retry(
                    batch_num, batch, self._send_request_to_endpoint, batch, endpoint_index
                )
                
                # æ›´æ–°ç»Ÿè®¡
                self.fault_processor.update_stats(batch_result)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­å¤„ç†
                if not self.fault_processor.should_continue_processing():
                    self.logger.error(f"âŒ å¤±è´¥ç‡è¿‡é«˜ï¼Œåœæ­¢å¤„ç†")
                    break
                
                batch_results = batch_result.results if batch_result.success else {}
                
            else:
                # æ™®é€šå¤„ç†
                try:
                    batch_results = await self._send_request_to_endpoint(batch, endpoint_index)
                except Exception as e:
                    self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
                    batch_results = {keyword: None for keyword in batch}
            
            # å¤„ç†ç»“æœ
            if batch_results:
                results.update(batch_results)
                
                # ç»Ÿè®¡æœ‰æ•ˆæ•°æ®
                valid_count = len([r for r in batch_results.values() if r is not None])
                self.total_processed_keywords += len(batch)
                
                if valid_count > 0:
                    self.logger.debug(f"âœ… æ‰¹æ¬¡ {batch_num}/{total_batches}: "
                                    f"{valid_count}/{len(batch)} ä¸ªæœ‰æ•ˆç»“æœ")
                    
                    # æ·»åŠ åˆ°ç¼“å†²åŒº
                    for keyword, data in batch_results.items():
                        if data:
                            keyword_data = {
                                "keyword": keyword,
                                "seo_data": data,
                                "timestamp": time.time()
                            }
                            storage_buffer.append(keyword_data)
                            submission_buffer.append(keyword_data)
                
                # å¢é‡ä¿å­˜æ£€æŸ¥
                if self.incremental_saver:
                    # æœ¬åœ°ä¿å­˜
                    if await self.incremental_saver.save_checkpoint(
                        self.total_processed_keywords, 
                        lambda: storage_callback(storage_buffer) if storage_callback and storage_buffer else None
                    ):
                        storage_buffer.clear()
                    
                    # Gitæäº¤
                    await self.incremental_saver.commit_to_git(self.total_processed_keywords)
                
                # æµå¼æäº¤æ£€æŸ¥
                if len(submission_buffer) >= 100 and submission_callback:
                    self.logger.info(f"ğŸ“¤ æµå¼æäº¤: {len(submission_buffer)} æ¡æ•°æ®")
                    await submission_callback(submission_buffer)
                    submission_buffer.clear()
            
            # ç­‰å¾…é—´éš”
            await asyncio.sleep(self.interval)
        
        # æœ€ç»ˆä¿å­˜
        await self._final_save(storage_buffer, submission_buffer, 
                             storage_callback, submission_callback)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._log_final_stats()
        
        return results
    
    async def _emergency_save(self, storage_buffer, submission_buffer,
                            storage_callback, submission_callback):
        """ç´§æ€¥ä¿å­˜"""
        self.logger.warning(f"ğŸš¨ æ‰§è¡Œç´§æ€¥ä¿å­˜")
        
        # ä¿å­˜ç¼“å†²åŒºæ•°æ®
        if storage_buffer and storage_callback:
            await storage_callback(storage_buffer)
            storage_buffer.clear()
        
        if submission_buffer and submission_callback:
            await submission_callback(submission_buffer)
            submission_buffer.clear()
        
        # å¢é‡ä¿å­˜å™¨ç´§æ€¥ä¿å­˜
        if self.incremental_saver:
            await self.incremental_saver.emergency_save(
                reason="æ¥è¿‘è¶…æ—¶é™åˆ¶"
            )
    
    async def _final_save(self, storage_buffer, submission_buffer,
                        storage_callback, submission_callback):
        """æœ€ç»ˆä¿å­˜"""
        # ä¿å­˜å‰©ä½™æ•°æ®
        if storage_buffer and storage_callback:
            self.logger.info(f"ğŸ’¾ æœ€ç»ˆå­˜å‚¨: {len(storage_buffer)} æ¡æ•°æ®")
            await storage_callback(storage_buffer)
        
        if submission_buffer and submission_callback:
            self.logger.info(f"ğŸ“¤ æœ€ç»ˆæäº¤: {len(submission_buffer)} æ¡æ•°æ®")
            await submission_callback(submission_buffer)
        
        # å¼ºåˆ¶æœ€ç»ˆä¿å­˜
        if self.incremental_saver:
            await self.incremental_saver.save_checkpoint(
                self.total_processed_keywords, force=True
            )
            await self.incremental_saver.commit_to_git(
                self.total_processed_keywords, force=True
            )
    
    def _log_final_stats(self):
        """è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        if self.fault_processor:
            self.logger.info(f"ğŸ“Š {self.fault_processor.get_stats_summary()}")
        
        if self.incremental_saver:
            self.logger.info(f"â±ï¸ {self.incremental_saver.get_progress_summary()}")
        
        if self.is_emergency_mode:
            self.logger.warning(f"âš ï¸ ä»»åŠ¡åœ¨ç´§æ€¥æ¨¡å¼ä¸‹ç»“æŸï¼ˆæ¥è¿‘è¶…æ—¶ï¼‰")
        else:
            self.logger.info(f"âœ… ä»»åŠ¡æ­£å¸¸å®Œæˆ")
    
    def get_failed_keywords(self) -> Set[str]:
        """è·å–å¤±è´¥çš„å…³é”®è¯åˆ—è¡¨"""
        if self.fault_processor:
            return self.fault_processor.get_failed_keywords()
        return set()
    
    def get_enhanced_statistics(self) -> Dict[str, Any]:
        """è·å–å¢å¼ºç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_statistics()
        
        if self.fault_processor:
            stats['fault_tolerance'] = {
                'total_batches': self.fault_processor.stats.total_batches,
                'success_rate': self.fault_processor.stats.success_rate,
                'keyword_success_rate': self.fault_processor.stats.keyword_success_rate,
                'retried_batches': self.fault_processor.stats.retried_batches,
                'failed_keywords_count': len(self.get_failed_keywords())
            }
        
        if self.incremental_saver:
            runtime_info = self.incremental_saver.get_runtime_info()
            stats['runtime'] = {
                'elapsed_hours': runtime_info['elapsed_hours'],
                'remaining_hours': runtime_info['remaining_hours'],
                'is_approaching_timeout': runtime_info['is_approaching_timeout'],
                'total_processed': self.total_processed_keywords
            }
        
        stats['emergency_mode'] = self.is_emergency_mode
        
        return stats
