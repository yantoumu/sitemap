"""
SEO APIç®¡ç†å™¨
ç®¡ç†ä¸¤ä¸ªSEO APIæ¥å£ï¼Œç¡®ä¿ä¸¥æ ¼ä¸²è¡Œè¯·æ±‚ï¼Œæ”¯æŒæ•…éšœè½¬ç§»
"""

import asyncio
import aiohttp
import time
from typing import Dict, List, Optional, Any, Set
from datetime import datetime
import logging

from ..utils.log_security import LogSecurity


class SEOAPIManager:
    """SEO APIä¸²è¡Œç®¡ç†å™¨"""
    
    def __init__(self, api_urls: List[str], interval: float = 1.0,
                 batch_size: int = 5, timeout: int = 30):
        """
        åˆå§‹åŒ–SEO APIç®¡ç†å™¨ - å¢å¼ºç«¯ç‚¹ç®¡ç†

        Args:
            api_urls: APIç«¯ç‚¹URLåˆ—è¡¨
            interval: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
            batch_size: æ‰¹é‡æŸ¥è¯¢å¤§å°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.api_urls = api_urls
        self.interval = interval
        self.batch_size = batch_size
        self.timeout = timeout

        # ç«¯ç‚¹å¥åº·çŠ¶æ€è·Ÿè¸ª
        self.endpoint_health = {i: {'healthy': True, 'failures': 0, 'last_check': 0}
                               for i in range(len(api_urls))}
        self.max_failures = 3  # è¿ç»­å¤±è´¥é˜ˆå€¼
        self.health_check_interval = 300  # 5åˆ†é’Ÿé‡æ–°æ£€æŸ¥ä¸å¥åº·ç«¯ç‚¹

        # æ™ºèƒ½ç«¯ç‚¹é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨k3.seokey.vip
        self.current_api_index = self._select_best_endpoint()
        self.enable_failover = True  # å¯ç”¨æ•…éšœè½¬ç§»

        self.request_lock = asyncio.Lock()
        self.last_request_time = 0
        self.logger = logging.getLogger(__name__)

        # è®°å½•ä½¿ç”¨çš„ç«¯ç‚¹
        self.logger.info(f"ğŸ¯ é€‰æ‹©ç«¯ç‚¹: {self.api_urls[self.current_api_index]} (ç´¢å¼•: {self.current_api_index})")

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'api_switches': 0,
            'total_keywords_queried': 0,
            'endpoint_failures': {i: 0 for i in range(len(api_urls))}
        }

        self.logger.info(f"SEO APIç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ŒAPIç«¯ç‚¹: {len(api_urls)}ä¸ªï¼Œæ•…éšœè½¬ç§»: å¯ç”¨")

    def _select_best_endpoint(self) -> int:
        """æ™ºèƒ½é€‰æ‹©æœ€ä½³ç«¯ç‚¹"""
        # ä¼˜å…ˆé€‰æ‹©k3.seokey.vipï¼ˆå·²çŸ¥ç¨³å®šï¼‰
        for i, url in enumerate(self.api_urls):
            if 'k3.seokey.vip' in url and self.endpoint_health[i]['healthy']:
                return i

        # å¦‚æœk3ä¸å¯ç”¨ï¼Œé€‰æ‹©å…¶ä»–å¥åº·ç«¯ç‚¹
        for i, health in self.endpoint_health.items():
            if health['healthy']:
                return i

        # å¦‚æœéƒ½ä¸å¥åº·ï¼Œé€‰æ‹©å¤±è´¥æ¬¡æ•°æœ€å°‘çš„
        return min(self.endpoint_health.keys(),
                  key=lambda x: self.endpoint_health[x]['failures'])

    def _mark_endpoint_failure(self, endpoint_index: int):
        """æ ‡è®°ç«¯ç‚¹å¤±è´¥"""
        import time
        health = self.endpoint_health[endpoint_index]
        health['failures'] += 1
        self.stats['endpoint_failures'][endpoint_index] += 1

        if health['failures'] >= self.max_failures:
            health['healthy'] = False
            health['last_check'] = time.time()
            self.logger.warning(f"ç«¯ç‚¹ {self.api_urls[endpoint_index]} æ ‡è®°ä¸ºä¸å¥åº· (è¿ç»­å¤±è´¥ {health['failures']} æ¬¡)")

    def _try_failover(self) -> bool:
        """å°è¯•æ•…éšœè½¬ç§»åˆ°å…¶ä»–ç«¯ç‚¹"""
        if not self.enable_failover:
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¥åº·ç«¯ç‚¹
        import time
        current_time = time.time()
        for i, health in self.endpoint_health.items():
            if i == self.current_api_index:
                continue

            # é‡æ–°æ£€æŸ¥ä¹‹å‰ä¸å¥åº·çš„ç«¯ç‚¹
            if not health['healthy'] and (current_time - health['last_check']) > self.health_check_interval:
                health['healthy'] = True
                health['failures'] = 0
                self.logger.info(f"é‡æ–°å¯ç”¨ç«¯ç‚¹ {self.api_urls[i]} (å¥åº·æ£€æŸ¥é—´éš”å·²è¿‡)")

            if health['healthy']:
                old_endpoint = self.api_urls[self.current_api_index]
                self.current_api_index = i
                self.stats['api_switches'] += 1
                self.logger.warning(f"ğŸ”„ æ•…éšœè½¬ç§»: {old_endpoint} â†’ {self.api_urls[i]}")
                return True

        return False

    async def query_keywords_serial(self, keywords: List[str]) -> Dict[str, Dict]:
        """
        ä¸²è¡ŒæŸ¥è¯¢å…³é”®è¯ï¼Œç¡®ä¿è¯·æ±‚é—´éš”
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            Dict[str, Dict]: å…³é”®è¯åˆ°æ•°æ®çš„æ˜ å°„
        """
        if not keywords:
            return {}
        
        async with self.request_lock:
            # å‡å°‘å†—ä½™æ—¥å¿—ï¼šåªåœ¨å¤§é‡å…³é”®è¯æ—¶æ˜¾ç¤ºINFOçº§åˆ«
            if len(keywords) > 50:
                self.logger.info(f"å¼€å§‹ä¸²è¡ŒæŸ¥è¯¢ {len(keywords)} ä¸ªå…³é”®è¯")
            else:
                self.logger.debug(f"å¼€å§‹ä¸²è¡ŒæŸ¥è¯¢ {len(keywords)} ä¸ªå…³é”®è¯")
            
            # è®¡ç®—ç­‰å¾…æ—¶é—´
            current_time = time.time()
            time_since_last = current_time - self.last_request_time
            
            if time_since_last < self.interval:
                wait_time = self.interval - time_since_last
                self.logger.info(f"ç­‰å¾… {wait_time:.2f} ç§’ä»¥æ»¡è¶³APIé™æµè¦æ±‚")
                await asyncio.sleep(wait_time)
            
            # æ‰¹é‡å¤„ç†
            results = {}
            for i in range(0, len(keywords), self.batch_size):
                batch = keywords[i:i + self.batch_size]
                
                # å°è¯•å½“å‰API
                try:
                    batch_results = await self._send_request(batch)
                    results.update(batch_results)
                    self.last_request_time = time.time()
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.stats['successful_requests'] += 1
                    self.stats['total_keywords_queried'] += len(batch)
                    
                except Exception as e:
                    self.logger.error(f"API {self.current_api_index} è¯·æ±‚å¤±è´¥: {e}")
                    self.stats['failed_requests'] += 1

                    # ä¸åˆ‡æ¢ç«¯ç‚¹ï¼Œç›´æ¥è®°å½•å¤±è´¥çš„å…³é”®è¯
                    for keyword in batch:
                        results[keyword] = None
                
                # æ›´æ–°æ€»è¯·æ±‚æ•°
                self.stats['total_requests'] += 1
                
                # æ‰¹æ¬¡é—´ç­‰å¾…
                if i + self.batch_size < len(keywords):
                    await asyncio.sleep(self.interval)
            
            self.logger.info(f"ä¸²è¡ŒæŸ¥è¯¢å®Œæˆï¼ŒæˆåŠŸ: {len([r for r in results.values() if r])}/{len(keywords)}")
            return results

    async def query_keywords_streaming(self, keywords: List[str],
                                     url_keywords_map: Dict[str, Set[str]] = None,
                                     storage_callback=None,
                                     submission_callback=None) -> Dict[str, Dict]:
        """
        æµå¼æŸ¥è¯¢å…³é”®è¯ï¼Œæ”¯æŒå®æ—¶å­˜å‚¨å’Œæäº¤

        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            url_keywords_map: URLåˆ°å…³é”®è¯çš„æ˜ å°„å…³ç³»
            storage_callback: å­˜å‚¨å›è°ƒå‡½æ•° async def(keyword_data_list)
            submission_callback: æäº¤å›è°ƒå‡½æ•° async def(keyword_data_list)

        Returns:
            Dict[str, Dict]: å…³é”®è¯åˆ°æ•°æ®çš„æ˜ å°„
        """
        if not keywords:
            return {}

        self.logger.info(f"ğŸ” SEOæ•°æ®æŸ¥è¯¢: {len(keywords)} ä¸ªå…³é”®è¯")

        # æµå¼å¤„ç†ç¼“å†²åŒº
        storage_buffer = []
        submission_buffer = []
        results = {}
        processed_count = 0

        # ä¿®å¤åˆå§‹åŒ–é—®é¢˜ï¼šè®¾ç½®æ­£ç¡®çš„åˆå§‹æ—¶é—´
        if self.last_request_time == 0:
            self.last_request_time = time.time() - self.interval

        async with self.request_lock:
            self.logger.info(f"ğŸ”„ å¼€å§‹å¤„ç† {len(keywords)} ä¸ªå…³é”®è¯ï¼Œåˆ†ä¸º {(len(keywords) + self.batch_size - 1) // self.batch_size} ä¸ªæ‰¹æ¬¡")

            for i in range(0, len(keywords), self.batch_size):
                batch = keywords[i:i + self.batch_size]
                batch_num = i // self.batch_size + 1

                # æ‰¹æ¬¡å¤„ç†æ—¥å¿—ï¼ˆä»…è°ƒè¯•æ¨¡å¼ï¼‰
                self.logger.debug(f"ğŸ“‹ å¤„ç†æ‰¹æ¬¡ {batch_num}: {len(batch)} ä¸ªå…³é”®è¯")

                # è®¡ç®—ç­‰å¾…æ—¶é—´ - æ·»åŠ ä¿æŠ¤æœºåˆ¶
                current_time = time.time()
                time_since_last = current_time - self.last_request_time

                if time_since_last < self.interval:
                    wait_time = min(self.interval - time_since_last, self.interval)  # é™åˆ¶æœ€å¤§ç­‰å¾…æ—¶é—´
                    # ç­‰å¾…æ—¶é—´æ—¥å¿—ï¼ˆä»…è°ƒè¯•æ¨¡å¼ï¼‰
                    self.logger.debug(f"â±ï¸ ç­‰å¾… {wait_time:.2f} ç§’")
                    await asyncio.sleep(wait_time)

                # å°è¯•å½“å‰APIï¼Œæ”¯æŒæ•…éšœè½¬ç§»
                batch_results = None
                max_retries = len(self.api_urls)  # æœ€å¤šå°è¯•æ‰€æœ‰ç«¯ç‚¹

                for retry in range(max_retries):
                    try:
                        current_endpoint = self.api_urls[self.current_api_index]
                        self.logger.debug(f"æŸ¥è¯¢æ‰¹æ¬¡ {i//self.batch_size + 1}: {len(batch)} ä¸ªå…³é”®è¯ (ç«¯ç‚¹: {current_endpoint})")

                        batch_results = await self._send_request(batch)
                        self.logger.debug(f"æ‰¹æ¬¡æŸ¥è¯¢å®Œæˆ: æ”¶åˆ° {len(batch_results)} ä¸ªç»“æœ")

                        # æˆåŠŸæ—¶é‡ç½®å¤±è´¥è®¡æ•°
                        self.endpoint_health[self.current_api_index]['failures'] = 0
                        break

                    except Exception as e:
                        self.logger.error(f"API {retry + 1} è¯·æ±‚å¤±è´¥: {e}")

                        # æ ‡è®°å½“å‰ç«¯ç‚¹å¤±è´¥
                        self._mark_endpoint_failure(self.current_api_index)

                        # å°è¯•æ•…éšœè½¬ç§»
                        if retry < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                            if self._try_failover():
                                self.logger.info(f"æ•…éšœè½¬ç§»æˆåŠŸï¼Œé‡è¯•æ‰¹æ¬¡ {i//self.batch_size + 1}")
                                continue
                            else:
                                self.logger.error("æ— å¯ç”¨çš„å¥åº·ç«¯ç‚¹ï¼Œåœæ­¢é‡è¯•")
                                break
                        else:
                            self.logger.error(f"æ‰€æœ‰ç«¯ç‚¹éƒ½å·²å°è¯•ï¼Œæ‰¹æ¬¡ {i//self.batch_size + 1} å¤±è´¥")

                if batch_results is not None:
                    results.update(batch_results)
                    self.last_request_time = time.time()

                    # æµå¼å¤„ç†ï¼šæ”¶é›†æˆåŠŸçš„æ•°æ®
                    valid_count = 0
                    invalid_count = 0

                    for keyword, data in batch_results.items():
                        if data:  # åªå¤„ç†æˆåŠŸçš„æ•°æ®
                            processed_count += 1
                            valid_count += 1

                            keyword_data = {
                                "keyword": keyword,
                                "seo_data": data,
                                "timestamp": time.time()
                            }

                            # æ·»åŠ åˆ°ç¼“å†²åŒº
                            storage_buffer.append(keyword_data)
                            submission_buffer.append(keyword_data)

                            self.logger.debug(f"âœ… æœ‰æ•ˆæ•°æ®: {keyword} -> {data}")
                        else:
                            invalid_count += 1
                            self.logger.debug(f"ğŸ“­ æ— æœç´¢æ•°æ®: {keyword} (APIç¡®è®¤è¯¥å…³é”®è¯æ— æœç´¢é‡æ•°æ®)")

                    # æ‰¹æ¬¡ç»“æœæ—¥å¿—ï¼ˆä»…è°ƒè¯•æ¨¡å¼ï¼‰
                    self.logger.debug(f"ğŸ“Š æ‰¹æ¬¡ {batch_num} ç»“æœ: æœ‰æ•ˆ {valid_count}, æ— æ•ˆ {invalid_count}, ç¼“å†²åŒº: å­˜å‚¨{len(storage_buffer)}/æäº¤{len(submission_buffer)}")

                    # ç´¯è®¡è¿›åº¦æ—¥å¿—ï¼ˆä»…è°ƒè¯•æ¨¡å¼ï¼‰
                    if processed_count > 0 and processed_count % 100 == 0:
                        self.logger.debug(f"ğŸ“ˆ ç´¯è®¡å¤„ç† {processed_count} æ¡æœ‰æ•ˆæ•°æ®")

                    # æ¯5æ¡è§¦å‘å­˜å‚¨ (å¿«é€Ÿæµ‹è¯•)
                    if len(storage_buffer) >= 5 and storage_callback:
                        self.logger.debug(f"ğŸ’¾ è§¦å‘å­˜å‚¨: {len(storage_buffer)} æ¡æ•°æ®")
                        await storage_callback(storage_buffer.copy())
                        storage_buffer.clear()

                    # æ¯5æ¡è§¦å‘æäº¤ (å¿«é€Ÿæµ‹è¯•)
                    if len(submission_buffer) >= 5 and submission_callback:
                        self.logger.debug(f"ğŸ“¤ è§¦å‘æäº¤: {len(submission_buffer)} æ¡æ•°æ®")
                        await submission_callback(submission_buffer.copy())
                        submission_buffer.clear()

                    # æ›´æ–°ç»Ÿè®¡
                    self.stats['successful_requests'] += 1
                    self.stats['total_keywords_queried'] += len(batch)
                else:
                    # å¦‚æœæ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥ï¼Œä¸ºè¿™ä¸ªæ‰¹æ¬¡åˆ›å»ºç©ºç»“æœ
                    for keyword in batch:
                        results[keyword] = None
                    self.logger.error(f"æ‰¹æ¬¡ {i//self.batch_size + 1} å®Œå…¨å¤±è´¥ï¼Œæ‰€æœ‰å…³é”®è¯æ ‡è®°ä¸ºå¤±è´¥")
                    self.stats['failed_requests'] += 1

                self.stats['total_requests'] += 1

        # å¤„ç†å‰©ä½™ç¼“å†²åŒºæ•°æ®
        if storage_buffer and storage_callback:
            self.logger.info(f"ğŸ’¾ æœ€ç»ˆå­˜å‚¨: {len(storage_buffer)} æ¡æ•°æ®")
            await storage_callback(storage_buffer)

        if submission_buffer and submission_callback:
            self.logger.info(f"ğŸ“¤ æœ€ç»ˆæäº¤: {len(submission_buffer)} æ¡æ•°æ®")
            await submission_callback(submission_buffer)

        self.logger.info(f"æµå¼æŸ¥è¯¢å®Œæˆï¼Œå¤„ç† {processed_count} æ¡æˆåŠŸæ•°æ®ï¼Œæ€»æŸ¥è¯¢ {len(keywords)} ä¸ªå…³é”®è¯")
        return results
    
    async def _send_request(self, keywords: List[str]) -> Dict[str, Dict]:
        """
        å‘é€APIè¯·æ±‚
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            Dict[str, Dict]: æŸ¥è¯¢ç»“æœ
            
        Raises:
            Exception: è¯·æ±‚å¤±è´¥
        """
        if not keywords:
            return {}
        
        url = f"{self.api_urls[self.current_api_index]}/api/keywords"
        params = {"keyword": ",".join(keywords)}
        
        # å®Œæ•´çš„è¶…æ—¶é…ç½®
        timeout = aiohttp.ClientTimeout(
            total=self.timeout,      # æ€»è¶…æ—¶æ—¶é—´: 30ç§’
            connect=10,              # è¿æ¥è¶…æ—¶æ—¶é—´: 10ç§’ (å…³é”®!)
            sock_read=15,            # è¯»å–è¶…æ—¶æ—¶é—´: 15ç§’
            sock_connect=5           # socketè¿æ¥è¶…æ—¶: 5ç§’
        )
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            try:
                # ç®€åŒ–æ—¥å¿—
                self.logger.debug(f"HTTPè¯·æ±‚: {url}")
                self.logger.debug(f"å…³é”®è¯: {list(keywords)}")

                async with session.get(url, params=params) as response:
                    self.logger.debug(f"HTTPå“åº”: {response.status}")
                    if response.status == 200:
                        data = await response.json()
                        # æ·»åŠ åŸå§‹å“åº”è°ƒè¯•
                        self.logger.debug(f"ğŸ” APIåŸå§‹å“åº”: {data}")
                        result = self._parse_response(data, keywords)
                        valid_count = len([r for r in result.values() if r])
                        self.logger.debug(f"ğŸ“Š è§£æç»“æœ: {valid_count}/{len(result)} ä¸ªæœ‰æ•ˆæ•°æ®")
                        return result
                    else:
                        error_text = await response.text()
                        self.logger.error(f"APIé”™è¯¯å“åº”: {response.status} - {error_text[:100]}{'...' if len(error_text) > 100 else ''}")
                        raise Exception(f"APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status} - {error_text}")
                        
            except asyncio.TimeoutError:
                raise Exception(f"è¯·æ±‚è¶…æ—¶ ({self.timeout}ç§’)")
            except aiohttp.ClientError as e:
                raise Exception(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
    
    def _parse_response(self, data: Any, keywords: List[str]) -> Dict[str, Dict]:
        """
        è§£æAPIå“åº”æ•°æ®

        Args:
            data: APIå“åº”æ•°æ®
            keywords: è¯·æ±‚çš„å…³é”®è¯åˆ—è¡¨

        Returns:
            Dict[str, Dict]: è§£æåçš„æ•°æ®ï¼ŒåŒ…å«çŠ¶æ€ä¿¡æ¯
        """
        results = {}

        try:
            # æ ¹æ®å®é™…APIå“åº”æ ¼å¼è§£æ
            if isinstance(data, dict) and 'data' in data and isinstance(data['data'], list):
                # å®é™…APIå“åº”æ ¼å¼: {"status": "success", "data": [{"keyword": "...", "metrics": {...}}]}

                # æ£€æŸ¥APIæ˜¯å¦ç¡®è®¤æ— æ•°æ®
                total_results = data.get('total_results', len(data['data']))
                api_status = data.get('status', 'unknown')

                self.logger.debug(f"APIå“åº”çŠ¶æ€: {api_status}, æ€»ç»“æœæ•°: {total_results}, æ•°æ®é¡¹æ•°: {len(data['data'])}")

                # å…ˆåˆå§‹åŒ–æ‰€æœ‰å…³é”®è¯ä¸ºNone
                for keyword in keywords:
                    results[keyword] = None

                # å¦‚æœAPIç¡®è®¤æ— æ•°æ®ï¼ˆtotal_results=0ä¸”dataä¸ºç©ºï¼‰
                if total_results == 0 and len(data['data']) == 0:
                    # è®°å½•APIç¡®è®¤æ— æ•°æ®çš„å…³é”®è¯
                    for keyword in keywords:
                        self.logger.debug(f"ğŸ“­ APIç¡®è®¤æ— æ•°æ®: {keyword} (total_results=0)")
                else:
                    # è§£ædataæ•°ç»„ä¸­çš„å…³é”®è¯æ•°æ®
                    for item in data['data']:
                        if isinstance(item, dict) and 'keyword' in item and 'metrics' in item:
                            keyword = item['keyword']
                            metrics = item['metrics']

                            # æ£€æŸ¥å…³é”®è¯æ˜¯å¦åœ¨è¯·æ±‚åˆ—è¡¨ä¸­
                            if keyword in keywords:
                                if self._is_valid_keyword_data(metrics):
                                    results[keyword] = metrics
                                    self.logger.debug(f"âœ… è§£ææˆåŠŸ: {keyword}")
                                else:
                                    self.logger.debug(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {keyword} -> {metrics}")
                            else:
                                self.logger.debug(f"âš ï¸ æ„å¤–çš„å…³é”®è¯: {keyword}")

                    # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®è¯åœ¨APIå“åº”ä¸­ç¼ºå¤±ï¼ˆéƒ¨åˆ†æ— æ•°æ®ï¼‰
                    returned_keywords = {item.get('keyword') for item in data['data']
                                       if isinstance(item, dict) and 'keyword' in item}
                    for keyword in keywords:
                        if keyword not in returned_keywords and results[keyword] is None:
                            self.logger.debug(f"ğŸ“­ APIæœªè¿”å›è¯¥å…³é”®è¯: {keyword}")

            else:
                # å¦‚æœå“åº”æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œæ ‡è®°æ‰€æœ‰å…³é”®è¯ä¸ºå¤±è´¥
                self.logger.warning(f"âš ï¸ APIå“åº”æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ: {type(data)}")
                for keyword in keywords:
                    results[keyword] = None

        except Exception as e:
            self.logger.error(f"è§£æAPIå“åº”å¤±è´¥: {e}")
            # è§£æå¤±è´¥ï¼Œæ ‡è®°æ‰€æœ‰å…³é”®è¯ä¸ºå¤±è´¥
            for keyword in keywords:
                results[keyword] = None

        return results
    
    def _is_valid_keyword_data(self, data: Any) -> bool:
        """
        éªŒè¯å…³é”®è¯æ•°æ®æ˜¯å¦æœ‰æ•ˆ - æ”¾å®½éªŒè¯æ¡ä»¶

        Args:
            data: å…³é”®è¯æ•°æ®

        Returns:
            bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        # æ”¾å®½éªŒè¯ï¼šåªè¦ä¸æ˜¯Noneä¸”æ˜¯dictå°±è®¤ä¸ºæœ‰æ•ˆ
        if data is None:
            return False

        if not isinstance(data, dict):
            return False

        # åªè¦æœ‰ä»»ä½•æ•°æ®å°±è®¤ä¸ºæœ‰æ•ˆ
        return len(data) > 0
    
    def switch_api(self) -> None:
        """åˆ‡æ¢åˆ°å¦ä¸€ä¸ªAPIç«¯ç‚¹"""
        old_index = self.current_api_index
        self.current_api_index = (self.current_api_index + 1) % len(self.api_urls)
        self.stats['api_switches'] += 1
        
        self.logger.info(f"åˆ‡æ¢APIç«¯ç‚¹: {old_index} -> {self.current_api_index}")
    
    def get_current_api_url(self) -> str:
        """
        è·å–å½“å‰ä½¿ç”¨çš„API URL
        
        Returns:
            str: å½“å‰API URL
        """
        return self.api_urls[self.current_api_index]
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        stats = self.stats.copy()
        stats['current_api_index'] = self.current_api_index
        stats['current_api_url'] = self.get_current_api_url()
        stats['success_rate'] = (
            self.stats['successful_requests'] / max(self.stats['total_requests'], 1) * 100
        )
        
        return stats
    
    def reset_statistics(self) -> None:
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'api_switches': 0,
            'total_keywords_queried': 0
        }
        self.logger.info("ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    async def health_check(self) -> Dict[str, bool]:
        """
        æ£€æŸ¥æ‰€æœ‰APIç«¯ç‚¹çš„å¥åº·çŠ¶æ€
        
        Returns:
            Dict[str, bool]: API URLåˆ°å¥åº·çŠ¶æ€çš„æ˜ å°„
        """
        health_status = {}
        
        for i, api_url in enumerate(self.api_urls):
            try:
                timeout = aiohttp.ClientTimeout(total=5)  # å¥åº·æ£€æŸ¥ä½¿ç”¨è¾ƒçŸ­è¶…æ—¶
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    # å‘é€ç®€å•çš„å¥åº·æ£€æŸ¥è¯·æ±‚
                    async with session.get(f"{api_url}/health") as response:
                        health_status[api_url] = response.status == 200
            except Exception:
                health_status[api_url] = False
        
        return health_status
