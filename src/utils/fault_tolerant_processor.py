"""
å®¹é”™å¤„ç†å™¨
å®ç°æ‰¹æ¬¡å¤±è´¥å®¹é”™ã€é‡è¯•æœºåˆ¶å’Œä¼˜é›…é™çº§
"""

import asyncio
import time
import random
from typing import List, Dict, Any, Optional, Callable, Set
from dataclasses import dataclass
import logging

from ..utils.log_security import LogSecurity


@dataclass
class BatchResult:
    """æ‰¹æ¬¡å¤„ç†ç»“æœ"""
    batch_id: int
    keywords: List[str]
    success: bool
    results: Dict[str, Any]
    error: Optional[str] = None
    retry_count: int = 0
    processing_time: float = 0.0


@dataclass
class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_batches: int = 0
    successful_batches: int = 0
    failed_batches: int = 0
    retried_batches: int = 0
    total_keywords: int = 0
    successful_keywords: int = 0
    failed_keywords: int = 0
    total_processing_time: float = 0.0
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        if self.total_batches == 0:
            return 0.0
        return self.successful_batches / self.total_batches
    
    @property
    def keyword_success_rate(self) -> float:
        """å…³é”®è¯æˆåŠŸç‡"""
        if self.total_keywords == 0:
            return 0.0
        return self.successful_keywords / self.total_keywords


class FaultTolerantProcessor:
    """å®¹é”™å¤„ç†å™¨"""
    
    def __init__(self,
                 max_retries: int = 3,
                 retry_delay_base: float = 1.0,
                 retry_delay_max: float = 60.0,
                 failure_threshold: float = 0.5,  # å¤±è´¥ç‡é˜ˆå€¼
                 circuit_breaker_threshold: int = 10,  # è¿ç»­å¤±è´¥é˜ˆå€¼
                 enable_exponential_backoff: bool = True):
        """
        åˆå§‹åŒ–å®¹é”™å¤„ç†å™¨
        
        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay_base: åŸºç¡€é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            retry_delay_max: æœ€å¤§é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            failure_threshold: å¤±è´¥ç‡é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è§¦å‘é™çº§
            circuit_breaker_threshold: è¿ç»­å¤±è´¥æ¬¡æ•°é˜ˆå€¼
            enable_exponential_backoff: æ˜¯å¦å¯ç”¨æŒ‡æ•°é€€é¿
        """
        self.max_retries = max_retries
        self.retry_delay_base = retry_delay_base
        self.retry_delay_max = retry_delay_max
        self.failure_threshold = failure_threshold
        self.circuit_breaker_threshold = circuit_breaker_threshold
        self.enable_exponential_backoff = enable_exponential_backoff
        
        self.stats = ProcessingStats()
        self.failed_batches: List[BatchResult] = []
        self.consecutive_failures = 0
        self.circuit_breaker_open = False
        self.circuit_breaker_open_time = 0.0
        self.circuit_breaker_timeout = 300.0  # 5åˆ†é’Ÿåé‡è¯•
        
        self.logger = logging.getLogger(__name__)
        
        self.logger.info(f"ğŸ›¡ï¸ å®¹é”™å¤„ç†å™¨åˆå§‹åŒ–: æœ€å¤§é‡è¯•={max_retries}, "
                        f"å¤±è´¥é˜ˆå€¼={failure_threshold:.1%}, "
                        f"ç†”æ–­é˜ˆå€¼={circuit_breaker_threshold}")
    
    def _calculate_retry_delay(self, retry_count: int) -> float:
        """è®¡ç®—é‡è¯•å»¶è¿Ÿ"""
        if not self.enable_exponential_backoff:
            return self.retry_delay_base
        
        # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
        delay = self.retry_delay_base * (2 ** retry_count)
        delay = min(delay, self.retry_delay_max)
        
        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼ˆÂ±20%ï¼‰
        jitter = delay * 0.2 * (random.random() - 0.5)
        delay += jitter
        
        return max(0.1, delay)
    
    def _should_open_circuit_breaker(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰“å¼€ç†”æ–­å™¨"""
        return self.consecutive_failures >= self.circuit_breaker_threshold
    
    def _should_close_circuit_breaker(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å…³é—­ç†”æ–­å™¨"""
        if not self.circuit_breaker_open:
            return False
        
        elapsed = time.time() - self.circuit_breaker_open_time
        return elapsed > self.circuit_breaker_timeout
    
    def _update_circuit_breaker(self, success: bool):
        """æ›´æ–°ç†”æ–­å™¨çŠ¶æ€"""
        if success:
            self.consecutive_failures = 0
            if self.circuit_breaker_open:
                self.circuit_breaker_open = False
                self.logger.info("ğŸ”“ ç†”æ–­å™¨å…³é—­ï¼Œæ¢å¤æ­£å¸¸å¤„ç†")
        else:
            self.consecutive_failures += 1
            
            if not self.circuit_breaker_open and self._should_open_circuit_breaker():
                self.circuit_breaker_open = True
                self.circuit_breaker_open_time = time.time()
                self.logger.warning(f"ğŸ”’ ç†”æ–­å™¨æ‰“å¼€: è¿ç»­å¤±è´¥ {self.consecutive_failures} æ¬¡")
    
    async def process_batch_with_retry(self,
                                     batch_id: int,
                                     keywords: List[str],
                                     process_func: Callable,
                                     *args, **kwargs) -> BatchResult:
        """
        å¸¦é‡è¯•çš„æ‰¹æ¬¡å¤„ç†
        
        Args:
            batch_id: æ‰¹æ¬¡ID
            keywords: å…³é”®è¯åˆ—è¡¨
            process_func: å¤„ç†å‡½æ•°
            *args, **kwargs: ä¼ é€’ç»™å¤„ç†å‡½æ•°çš„å‚æ•°
            
        Returns:
            BatchResult: æ‰¹æ¬¡å¤„ç†ç»“æœ
        """
        # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
        if self.circuit_breaker_open:
            if self._should_close_circuit_breaker():
                self.logger.info("ğŸ”„ å°è¯•å…³é—­ç†”æ–­å™¨")
            else:
                self.logger.warning(f"âš¡ ç†”æ–­å™¨å¼€å¯ï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_id}")
                return BatchResult(
                    batch_id=batch_id,
                    keywords=keywords,
                    success=False,
                    results={},
                    error="ç†”æ–­å™¨å¼€å¯"
                )
        
        start_time = time.time()
        last_error = None
        
        for retry_count in range(self.max_retries + 1):
            try:
                # å¦‚æœæ˜¯é‡è¯•ï¼Œå…ˆç­‰å¾…
                if retry_count > 0:
                    delay = self._calculate_retry_delay(retry_count - 1)
                    self.logger.info(f"ğŸ”„ æ‰¹æ¬¡ {batch_id} ç¬¬ {retry_count} æ¬¡é‡è¯• "
                                   f"(å»¶è¿Ÿ {delay:.1f}s)")
                    await asyncio.sleep(delay)
                
                # æ‰§è¡Œå¤„ç†å‡½æ•°
                results = await process_func(*args, **kwargs)
                
                # æˆåŠŸå¤„ç†
                processing_time = time.time() - start_time
                self._update_circuit_breaker(True)
                
                # ç»Ÿè®¡æˆåŠŸçš„å…³é”®è¯æ•°é‡
                successful_keywords = len([r for r in results.values() if r is not None])
                
                batch_result = BatchResult(
                    batch_id=batch_id,
                    keywords=keywords,
                    success=True,
                    results=results,
                    retry_count=retry_count,
                    processing_time=processing_time
                )
                
                if retry_count > 0:
                    self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_id} é‡è¯•æˆåŠŸ "
                                   f"(ç¬¬ {retry_count} æ¬¡é‡è¯•)")
                
                return batch_result
                
            except Exception as e:
                last_error = str(e)
                self.logger.warning(f"âŒ æ‰¹æ¬¡ {batch_id} å¤„ç†å¤±è´¥ "
                                  f"(å°è¯• {retry_count + 1}/{self.max_retries + 1}): {e}")
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œè®°å½•å¤±è´¥
                if retry_count == self.max_retries:
                    self._update_circuit_breaker(False)
                    break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        processing_time = time.time() - start_time
        
        batch_result = BatchResult(
            batch_id=batch_id,
            keywords=keywords,
            success=False,
            results={keyword: None for keyword in keywords},
            error=last_error,
            retry_count=self.max_retries,
            processing_time=processing_time
        )
        
        self.failed_batches.append(batch_result)
        
        return batch_result
    
    def update_stats(self, batch_result: BatchResult):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats.total_batches += 1
        self.stats.total_keywords += len(batch_result.keywords)
        self.stats.total_processing_time += batch_result.processing_time
        
        if batch_result.success:
            self.stats.successful_batches += 1
            # ç»Ÿè®¡æˆåŠŸçš„å…³é”®è¯
            successful_count = len([r for r in batch_result.results.values() if r is not None])
            self.stats.successful_keywords += successful_count
            self.stats.failed_keywords += len(batch_result.keywords) - successful_count
        else:
            self.stats.failed_batches += 1
            self.stats.failed_keywords += len(batch_result.keywords)
            
        if batch_result.retry_count > 0:
            self.stats.retried_batches += 1
    
    def should_continue_processing(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­å¤„ç†"""
        # å¦‚æœè¿˜æ²¡æœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œç»§ç»­å¤„ç†
        if self.stats.total_batches < 10:
            return True
        
        # æ£€æŸ¥å¤±è´¥ç‡æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if self.stats.success_rate < (1 - self.failure_threshold):
            self.logger.warning(f"âš ï¸ å¤±è´¥ç‡è¿‡é«˜: {(1-self.stats.success_rate):.1%}, "
                              f"é˜ˆå€¼: {self.failure_threshold:.1%}")
            return False
        
        return True
    
    def get_failed_keywords(self) -> Set[str]:
        """è·å–æ‰€æœ‰å¤±è´¥çš„å…³é”®è¯"""
        failed_keywords = set()
        for batch_result in self.failed_batches:
            failed_keywords.update(batch_result.keywords)
        return failed_keywords
    
    def get_stats_summary(self) -> str:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        return (f"å¤„ç†ç»Ÿè®¡: æ‰¹æ¬¡ {self.stats.successful_batches}/{self.stats.total_batches} "
               f"({self.stats.success_rate:.1%}), "
               f"å…³é”®è¯ {self.stats.successful_keywords}/{self.stats.total_keywords} "
               f"({self.stats.keyword_success_rate:.1%}), "
               f"é‡è¯• {self.stats.retried_batches} æ¬¡, "
               f"å¹³å‡è€—æ—¶ {self.stats.total_processing_time/max(1,self.stats.total_batches):.1f}s")
