"""
æ•°æ®å¤„ç†å™¨
è´Ÿè´£å…³é”®è¯æ•°æ®çš„æŸ¥è¯¢ã€ä¿å­˜å’Œæäº¤å¤„ç†
"""

import asyncio
import time
from typing import List, Set, Dict, Any
from datetime import datetime
import logging

from .api import SEOAPIManager, BackendAPIClient
from .api.keyword_data_transformer import KeywordDataTransformer
from .api.keyword_metrics_client import KeywordMetricsClient
from .storage import StorageManager
from .utils import get_logger, TimingLogger, ProgressLogger


class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ - è´Ÿè´£å…³é”®è¯æ•°æ®çš„å¤„ç†æµç¨‹"""
    
    def __init__(self, seo_api: SEOAPIManager, backend_api: BackendAPIClient,
                 storage: StorageManager, keyword_metrics_client: KeywordMetricsClient = None):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            seo_api: SEO APIç®¡ç†å™¨
            backend_api: åç«¯APIå®¢æˆ·ç«¯
            storage: å­˜å‚¨ç®¡ç†å™¨
            keyword_metrics_client: å…³é”®è¯æŒ‡æ ‡å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œç”¨äºæ–°APIï¼‰
        """
        self.seo_api = seo_api
        self.backend_api = backend_api
        self.storage = storage
        self.keyword_metrics_client = keyword_metrics_client
        self.data_transformer = KeywordDataTransformer()
        self.logger = get_logger(__name__)
    
    async def process_keywords_data(self, url_keywords_map: Dict[str, Set[str]]) -> Dict[str, Any]:
        """
        å¤„ç†å…³é”®è¯æ•°æ®çš„å®Œæ•´æµç¨‹ - å¼‚æ­¥å¹¶è¡Œå¤„ç†ç‰ˆæœ¬

        Args:
            url_keywords_map: URLåˆ°å…³é”®è¯é›†åˆçš„æ˜ å°„

        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœç»Ÿè®¡
        """
        # æ•°æ®ç±»å‹éªŒè¯å’Œè½¬æ¢
        url_keywords_map = self._validate_and_convert_url_keywords_map(url_keywords_map)

        if not url_keywords_map:
            return self._create_empty_result()

        # 1. è·å–æ‰€æœ‰å”¯ä¸€å…³é”®è¯ - æ·»åŠ ç±»å‹å®‰å…¨æ£€æŸ¥
        all_keywords = set()

        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿url_keywords_mapæ˜¯å­—å…¸ç±»å‹
        if not isinstance(url_keywords_map, dict):
            self.logger.error(f"âŒ url_keywords_mapç±»å‹é”™è¯¯: {type(url_keywords_map)}, æœŸæœ›dictç±»å‹")
            self.logger.error(f"âŒ æ•°æ®å†…å®¹: {str(url_keywords_map)[:200]}...")
            return self._create_empty_result()

        try:
            for keywords in url_keywords_map.values():
                if isinstance(keywords, (set, list, tuple)):
                    all_keywords.update(keywords)
                else:
                    self.logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆå…³é”®è¯ç±»å‹: {type(keywords)}")
        except AttributeError as e:
            self.logger.error(f"âŒ AttributeError: {e}")
            self.logger.error(f"âŒ url_keywords_mapç±»å‹: {type(url_keywords_map)}")
            self.logger.error(f"âŒ url_keywords_mapå†…å®¹: {str(url_keywords_map)[:500]}...")
            import traceback
            self.logger.error(f"âŒ å®Œæ•´å †æ ˆ: {traceback.format_exc()}")
            return self._create_empty_result()
        except Exception as e:
            self.logger.error(f"âŒ æå–å…³é”®è¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            self.logger.error(f"âŒ url_keywords_mapç±»å‹: {type(url_keywords_map)}")
            return self._create_empty_result()

        self.logger.info(f"å…±æå– {len(all_keywords)} ä¸ªå”¯ä¸€å…³é”®è¯")

        # 2. è¿‡æ»¤å·²å¤„ç†çš„å…³é”®è¯ï¼ˆå»é‡æ£€æŸ¥ï¼‰
        new_keywords = self._filter_processed_keywords(all_keywords)
        if len(new_keywords) < len(all_keywords):
            filtered_count = len(all_keywords) - len(new_keywords)
            self.logger.info(f"è¿‡æ»¤å·²å¤„ç†å…³é”®è¯: {filtered_count} ä¸ªï¼Œå‰©ä½™ {len(new_keywords)} ä¸ªå¾…å¤„ç†")

            # æ›´æ–°URLæ˜ å°„ï¼Œåªä¿ç•™æ–°å…³é”®è¯
            url_keywords_map = self._update_url_keywords_map(url_keywords_map, new_keywords)

            if not new_keywords:
                self.logger.info("æ‰€æœ‰å…³é”®è¯éƒ½å·²å¤„ç†ï¼Œè·³è¿‡æŸ¥è¯¢")
                return self._create_empty_result()
        else:
            self.logger.info("æ‰€æœ‰å…³é”®è¯éƒ½æ˜¯æ–°çš„ï¼Œæ— éœ€è¿‡æ»¤")

        # 3. æŸ¥è¯¢å…³é”®è¯æ•°æ®
        keyword_data = await self._query_keywords(list(new_keywords), url_keywords_map)

        # 4. ä¸¥æ ¼è¿‡æ»¤æˆåŠŸæ•°æ®
        successful_data = self._filter_successful_data(keyword_data, url_keywords_map)

        if not successful_data['keyword_data'] and not successful_data['url_keywords_map']:
            self.logger.warning("æ²¡æœ‰æˆåŠŸæŸ¥è¯¢çš„æ•°æ®ï¼Œè·³è¿‡åç»­å¤„ç†")
            return {
                'total_keywords': len(all_keywords),
                'successful_keywords': 0,
                'saved_urls': 0,
                'submitted_records': 0,
                'storage_success': False,
                'submit_success': False
            }

        # 5. å¹¶è¡Œå¤„ç†ï¼šç›´æ¥ä½¿ç”¨asyncio.gatheråŒæ—¶æ‰§è¡Œæœ¬åœ°å­˜å‚¨å’Œåç«¯æäº¤
        storage_result, submit_result = await self._execute_parallel_simple(
            successful_data['url_keywords_map'],
            successful_data['keyword_data']
        )

        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        successful_keywords = len(successful_data['keyword_data'])
        submitted_records = submit_result.get('submitted_count', 0)

        self.logger.info(f"ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
        self.logger.info(f"   âœ… æˆåŠŸæŸ¥è¯¢å…³é”®è¯: {successful_keywords} ä¸ª")
        self.logger.info(f"   âœ… æˆåŠŸæäº¤è®°å½•: {submitted_records} æ¡")

        return {
            'total_keywords': len(all_keywords),
            'successful_keywords': successful_keywords,
            'saved_urls': storage_result.get('saved_count', 0),
            'submitted_records': submitted_records,
            'storage_success': storage_result.get('success', False),
            'submit_success': submit_result.get('success', False),
            'storage_error': storage_result.get('error'),
            'submit_error': submit_result.get('error')
        }

    def _filter_processed_keywords(self, keywords: set) -> set:
        """
        è¿‡æ»¤å·²å¤„ç†çš„å…³é”®è¯

        Args:
            keywords: å…³é”®è¯é›†åˆ

        Returns:
            set: æœªå¤„ç†çš„å…³é”®è¯é›†åˆ
        """
        new_keywords = set()
        processed_count = 0

        for keyword in keywords:
            if not self.storage.is_keyword_processed(keyword):
                new_keywords.add(keyword)
            else:
                processed_count += 1

        if processed_count > 0:
            self.logger.debug(f"å‘ç° {processed_count} ä¸ªå·²å¤„ç†å…³é”®è¯")

        return new_keywords

    def _update_url_keywords_map(self, url_keywords_map: Dict[str, Set[str]],
                                new_keywords: set) -> Dict[str, Set[str]]:
        """
        æ›´æ–°URLå…³é”®è¯æ˜ å°„ï¼Œåªä¿ç•™æ–°å…³é”®è¯

        Args:
            url_keywords_map: åŸå§‹URLå…³é”®è¯æ˜ å°„
            new_keywords: æ–°å…³é”®è¯é›†åˆ

        Returns:
            Dict[str, Set[str]]: æ›´æ–°åçš„URLå…³é”®è¯æ˜ å°„
        """
        updated_map = {}

        for url, keywords in url_keywords_map.items():
            # åªä¿ç•™æ–°å…³é”®è¯
            url_new_keywords = keywords.intersection(new_keywords)
            if url_new_keywords:
                updated_map[url] = url_new_keywords

        return updated_map
    
    async def _query_keywords(self, keywords: List[str],
                                     url_keywords_map: Dict[str, Set[str]] = None) -> Dict[str, Dict]:
        """
        æŸ¥è¯¢æ‰€æœ‰å…³é”®è¯æ•°æ® - æµå¼å¤„ç†ç‰ˆæœ¬

        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            url_keywords_map: URLåˆ°å…³é”®è¯çš„æ˜ å°„å…³ç³»

        Returns:
            Dict[str, Dict]: å…³é”®è¯æ•°æ®æ˜ å°„
        """
        if not keywords:
            return {}

        # åˆ›å»ºæµå¼å­˜å‚¨å’Œæäº¤å›è°ƒ
        async def storage_callback(keyword_data_list):
            """æµå¼å­˜å‚¨å›è°ƒ - ç®€åŒ–æ—¥å¿—"""
            try:
                from datetime import datetime

                self.logger.debug(f"ğŸ’¾ æœ¬åœ°å­˜å‚¨: ä¿å­˜ {len(keyword_data_list)} æ¡æ•°æ®")

                saved_count = 0
                for data in keyword_data_list:
                    keyword = data['keyword']
                    seo_data = data['seo_data']

                    # ä¿å­˜å·²å¤„ç†çš„å…³é”®è¯ï¼ˆä»…ä¿å­˜åŠ å¯†æ ‡è¯†ï¼‰
                    success = await self.storage.save_processed_keyword(keyword)
                    if success:
                        saved_count += 1
                    else:
                        self.logger.debug(f"å…³é”®è¯ {keyword} å­˜å‚¨å¤±è´¥")

                self.logger.debug(f"ğŸ’¾ æœ¬åœ°å­˜å‚¨å®Œæˆ: æˆåŠŸä¿å­˜ {saved_count}/{len(keyword_data_list)} æ¡æ•°æ®")

            except Exception as e:
                self.logger.error(f"âŒ æœ¬åœ°å­˜å‚¨å¤±è´¥: {e}")

        async def submission_callback(keyword_data_list):
            """æµå¼æäº¤å›è°ƒ - è¯¦ç»†åç«¯APIæ—¥å¿—"""
            try:
                from datetime import datetime

                # åç«¯æäº¤æ—¥å¿—ï¼ˆä»…è°ƒè¯•æ¨¡å¼ï¼‰
                self.logger.debug(f"ğŸš€ åç«¯APIæäº¤: {len(keyword_data_list)} æ¡æ•°æ®")

                # å‡†å¤‡æäº¤æ•°æ® - è½¬æ¢ä¸ºæ­£ç¡®çš„APIæ ¼å¼ï¼ŒåŒ…å«URLä¿¡æ¯
                keyword_data_dict = {}
                for data in keyword_data_list:
                    keyword_data_dict[data['keyword']] = data['seo_data']

                # ä½¿ç”¨æ­£ç¡®çš„æ•°æ®æ ¼å¼è½¬æ¢æ–¹æ³•ï¼Œä¼ é€’URLæ˜ å°„ä¿¡æ¯
                submit_data = self._prepare_legacy_submit_data(keyword_data_dict, url_keywords_map)

                # æäº¤åˆ°åç«¯
                start_time = time.time()
                if self.keyword_metrics_client:
                    success = await self.keyword_metrics_client.submit_keyword_metrics_batch(submit_data)
                    api_type = "æ–°API (keyword-metrics)"
                elif self.backend_api:
                    success = await self.backend_api.submit_batch(submit_data)
                    api_type = "åç«¯API (work.seokey.vip)"
                else:
                    success = False
                    api_type = "æœªé…ç½®"
                    self.logger.warning("âŒ æ²¡æœ‰é…ç½®åç«¯APIå®¢æˆ·ç«¯")

                end_time = time.time()
                duration = end_time - start_time

                # æäº¤ç»“æœæ—¥å¿—ï¼ˆä»…è°ƒè¯•æ¨¡å¼ï¼‰
                if success:
                    self.logger.debug(f"âœ… åç«¯APIæäº¤æˆåŠŸ: {len(keyword_data_list)} æ¡æ•°æ®")
                else:
                    self.logger.error(f"âŒ åç«¯APIæäº¤å¤±è´¥:")
                    self.logger.error(f"   APIç±»å‹: {api_type}")
                    self.logger.error(f"   æäº¤çŠ¶æ€: å¤±è´¥")
                    self.logger.error(f"   æ•°æ®é‡: {len(keyword_data_list)} æ¡")
                    self.logger.error(f"   è€—æ—¶: {duration:.2f} ç§’")

            except Exception as e:
                self.logger.error(f"âŒ åç«¯APIæäº¤å¼‚å¸¸: {e}")
                import traceback
                self.logger.debug(f"å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

        with TimingLogger(self.logger, f"å¼¹æ€§æŸ¥è¯¢ {len(keywords)} ä¸ªå…³é”®è¯"):
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¢å¼ºç‰ˆAPIç®¡ç†å™¨
            if hasattr(self.seo_api, 'query_keywords_with_resilience'):
                self.logger.info("ğŸš€ ä½¿ç”¨å¢å¼ºç‰ˆå¼¹æ€§æŸ¥è¯¢")
                return await self.seo_api.query_keywords_with_resilience(
                    keywords,
                    storage_callback=storage_callback,
                    submission_callback=submission_callback
                )
            else:
                self.logger.info("ğŸ“¡ ä½¿ç”¨æ ‡å‡†æµå¼æŸ¥è¯¢")
                return await self.seo_api.query_keywords_streaming(
                    keywords,
                    url_keywords_map,
                    storage_callback=storage_callback,
                    submission_callback=submission_callback
                )

    def _filter_successful_data(self, keyword_data: Dict[str, Dict],
                               url_keywords_map: Dict[str, Set[str]]) -> Dict[str, Any]:
        """
        ä¸¥æ ¼è¿‡æ»¤æˆåŠŸæ•°æ®ï¼Œä¸¢å¼ƒæŸ¥è¯¢å¤±è´¥çš„æ•°æ®

        Args:
            keyword_data: åŸå§‹å…³é”®è¯æ•°æ®
            url_keywords_map: åŸå§‹URLå…³é”®è¯æ˜ å°„

        Returns:
            Dict[str, Any]: è¿‡æ»¤åçš„æˆåŠŸæ•°æ®
        """
        # é˜²å¾¡æ€§ç±»å‹æ£€æŸ¥ï¼šç¡®ä¿url_keywords_mapæ˜¯å­—å…¸ç±»å‹
        if not isinstance(url_keywords_map, dict):
            self.logger.error(f"url_keywords_mapç±»å‹é”™è¯¯: æœŸæœ›dictï¼Œå®é™…{type(url_keywords_map)}")
            raise TypeError(f"url_keywords_mapå¿…é¡»æ˜¯å­—å…¸ç±»å‹ï¼Œå®é™…ç±»å‹: {type(url_keywords_map)}")

        # é˜²å¾¡æ€§ç±»å‹æ£€æŸ¥ï¼šç¡®ä¿keyword_dataæ˜¯å­—å…¸ç±»å‹
        if not isinstance(keyword_data, dict):
            self.logger.error(f"keyword_dataç±»å‹é”™è¯¯: æœŸæœ›dictï¼Œå®é™…{type(keyword_data)}")
            raise TypeError(f"keyword_dataå¿…é¡»æ˜¯å­—å…¸ç±»å‹ï¼Œå®é™…ç±»å‹: {type(keyword_data)}")

        # è¿‡æ»¤å‡ºæˆåŠŸæŸ¥è¯¢çš„å…³é”®è¯
        successful_keywords = {k: v for k, v in keyword_data.items() if v}

        # è¿‡æ»¤URLæ˜ å°„ï¼Œåªä¿ç•™æœ‰æˆåŠŸå…³é”®è¯çš„URL
        successful_url_keywords_map = {}
        for url, keywords in url_keywords_map.items():
            # æ‰¾å‡ºè¯¥URLä¸‹æˆåŠŸæŸ¥è¯¢çš„å…³é”®è¯
            url_successful_keywords = keywords.intersection(successful_keywords.keys())
            if url_successful_keywords:
                successful_url_keywords_map[url] = url_successful_keywords

        self.logger.info(f"æ•°æ®è¿‡æ»¤å®Œæˆ: {len(successful_keywords)}/{len(keyword_data)} ä¸ªå…³é”®è¯æˆåŠŸ, "
                        f"{len(successful_url_keywords_map)}/{len(url_keywords_map)} ä¸ªURLæœ‰æ•ˆ")

        return {
            'keyword_data': successful_keywords,
            'url_keywords_map': successful_url_keywords_map
        }

    async def _execute_parallel_simple(self, url_keywords_map: Dict[str, Set[str]],
                                      keyword_data: Dict[str, Dict]) -> tuple:
        """
        ç®€å•å¹¶è¡Œå¤„ç†ï¼šç›´æ¥ä½¿ç”¨asyncio.gather

        Args:
            url_keywords_map: æˆåŠŸçš„URLå…³é”®è¯æ˜ å°„
            keyword_data: æˆåŠŸçš„å…³é”®è¯æ•°æ®

        Returns:
            tuple: (storage_result, submit_result)
        """
        self.logger.info("å¼€å§‹å¹¶è¡Œå¤„ç†ï¼šæœ¬åœ°å­˜å‚¨ + åç«¯æäº¤")

        # ç›´æ¥å¹¶è¡Œæ‰§è¡Œï¼Œæ— éœ€æ•°æ®å‰¯æœ¬ï¼ˆæ•°æ®åªè¯»ï¼‰
        storage_task = self._storage_simple(url_keywords_map, keyword_data)
        submit_task = self._submit_simple(keyword_data, url_keywords_map)

        # å¹¶è¡Œæ‰§è¡Œï¼Œæ”¶é›†å¼‚å¸¸
        results = await asyncio.gather(storage_task, submit_task, return_exceptions=True)

        # å¤„ç†ç»“æœ
        storage_result = results[0] if not isinstance(results[0], Exception) else {
            'success': False, 'saved_count': 0, 'error': f"{type(results[0]).__name__}: {str(results[0])}"
        }
        submit_result = results[1] if not isinstance(results[1], Exception) else {
            'success': False, 'submitted_count': 0, 'error': f"{type(results[1]).__name__}: {str(results[1])}"
        }

        # è®°å½•å¼‚å¸¸è¯¦æƒ…
        if isinstance(results[0], Exception):
            self.logger.error(f"å­˜å‚¨ä»»åŠ¡å¼‚å¸¸: {type(results[0]).__name__}: {results[0]}")
        if isinstance(results[1], Exception):
            self.logger.error(f"æäº¤ä»»åŠ¡å¼‚å¸¸: {type(results[1]).__name__}: {results[1]}")

        self.logger.info(f"å¹¶è¡Œå¤„ç†å®Œæˆ - "
                       f"å­˜å‚¨: {'æˆåŠŸ' if storage_result.get('success') else 'å¤±è´¥'}, "
                       f"æäº¤: {'æˆåŠŸ' if submit_result.get('success') else 'å¤±è´¥'}")

        return storage_result, submit_result

    async def _storage_simple(self, url_keywords_map: Dict[str, Set[str]],
                            keyword_data: Dict[str, Dict]) -> Dict[str, Any]:
        """
        ç®€åŒ–çš„å­˜å‚¨ä»»åŠ¡

        Args:
            url_keywords_map: URLå…³é”®è¯æ˜ å°„
            keyword_data: å…³é”®è¯æ•°æ®

        Returns:
            Dict[str, Any]: å­˜å‚¨ç»“æœ
        """
        try:
            self.logger.info(f"å­˜å‚¨ä»»åŠ¡å¼€å§‹: {len(url_keywords_map)} ä¸ªURL")

            saved_count = 0
            for url, keywords in url_keywords_map.items():
                # æ„å»ºè¯¥URLçš„SEOæ•°æ®ï¼ˆç›´æ¥ä½¿ç”¨åŸæ•°æ®ï¼Œæ— éœ€å‰¯æœ¬ï¼‰
                url_seo_data = {
                    keyword: keyword_data[keyword]
                    for keyword in keywords
                    if keyword in keyword_data and keyword_data[keyword]
                }

                if url_seo_data:
                    success = await self.storage.save_processed_url(
                        url, list(keywords), url_seo_data
                    )
                    if success:
                        saved_count += 1

            self.logger.info(f"å­˜å‚¨ä»»åŠ¡å®Œæˆ: æˆåŠŸä¿å­˜ {saved_count} ä¸ªURL")
            return {'success': True, 'saved_count': saved_count}

        except Exception as e:
            self.logger.error(f"å­˜å‚¨ä»»åŠ¡å¤±è´¥: {e}")
            return {'success': False, 'saved_count': 0, 'error': str(e)}

    async def _submit_simple(self, keyword_data: Dict[str, Dict],
                           url_keywords_map: Dict[str, Set[str]]) -> Dict[str, Any]:
        """
        ç®€åŒ–çš„æäº¤ä»»åŠ¡

        Args:
            keyword_data: å…³é”®è¯æ•°æ®
            url_keywords_map: URLå…³é”®è¯æ˜ å°„

        Returns:
            Dict[str, Any]: æäº¤ç»“æœ
        """
        try:
            self.logger.info(f"æäº¤ä»»åŠ¡å¼€å§‹: {len(keyword_data)} ä¸ªå…³é”®è¯")

            # å‡†å¤‡æäº¤æ•°æ®ï¼ˆç›´æ¥ä½¿ç”¨åŸæ•°æ®ï¼Œæ— éœ€å‰¯æœ¬ï¼‰
            submit_data = await self._prepare_submit_data(keyword_data, url_keywords_map)

            if not submit_data:
                self.logger.info("æäº¤ä»»åŠ¡: æ²¡æœ‰æ•°æ®éœ€è¦æäº¤")
                return {'success': True, 'submitted_count': 0}

            # æäº¤åˆ°åç«¯
            success = await self._submit_to_backend(submit_data, url_keywords_map)
            submitted_count = len(submit_data) if success else 0

            self.logger.info(f"æäº¤ä»»åŠ¡å®Œæˆ: {'æˆåŠŸ' if success else 'å¤±è´¥'} "
                           f"æäº¤ {submitted_count} æ¡è®°å½•")

            return {'success': success, 'submitted_count': submitted_count}

        except Exception as e:
            self.logger.error(f"æäº¤ä»»åŠ¡å¤±è´¥: {e}")
            return {'success': False, 'submitted_count': 0, 'error': str(e)}

    async def _prepare_submit_data(self, keyword_data: Dict[str, Dict],
                                  url_keywords_map: Dict[str, Set[str]]) -> List[Dict]:
        """
        å‡†å¤‡æäº¤æ•°æ®

        Args:
            keyword_data: å…³é”®è¯æ•°æ®
            url_keywords_map: URLåˆ°å…³é”®è¯é›†åˆçš„æ˜ å°„

        Returns:
            List[Dict]: æäº¤æ•°æ®åˆ—è¡¨
        """
        if self.keyword_metrics_client:
            # ä½¿ç”¨æ–°çš„APIæ ¼å¼
            return await self._prepare_new_api_submit_data(keyword_data, url_keywords_map)
        else:
            # ä½¿ç”¨æ—§çš„APIæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            return self._prepare_legacy_submit_data(keyword_data)

    async def _save_successful_data(self, url_keywords_map: Dict[str, Set[str]],
                                   keyword_data: Dict[str, Dict]) -> int:
        """
        ä¿å­˜æˆåŠŸæŸ¥è¯¢çš„æ•°æ®
        
        Args:
            url_keywords_map: URLå…³é”®è¯æ˜ å°„
            keyword_data: å…³é”®è¯æ•°æ®
            
        Returns:
            int: ä¿å­˜çš„URLæ•°é‡
        """
        saved_count = 0
        
        for url, keywords in url_keywords_map.items():
            # æ„å»ºè¯¥URLçš„SEOæ•°æ®
            url_seo_data = {}
            for keyword in keywords:
                if keyword in keyword_data and keyword_data[keyword]:
                    url_seo_data[keyword] = keyword_data[keyword]
            
            # åªæœ‰æœ‰æˆåŠŸæ•°æ®æ‰ä¿å­˜
            if url_seo_data:
                success = await self.storage.save_processed_url(
                    url, list(keywords), url_seo_data
                )
                if success:
                    saved_count += 1
        
        self.logger.info(f"æˆåŠŸä¿å­˜ {saved_count} ä¸ªURLçš„æ•°æ®")
        return saved_count
    
    async def _prepare_submit_data(self, keyword_data: Dict[str, Dict],
                                  url_keywords_map: Dict[str, Set[str]]) -> List[Dict]:
        """
        å‡†å¤‡æäº¤æ•°æ®

        Args:
            keyword_data: å…³é”®è¯æ•°æ®
            url_keywords_map: URLåˆ°å…³é”®è¯é›†åˆçš„æ˜ å°„

        Returns:
            List[Dict]: æäº¤æ•°æ®åˆ—è¡¨
        """
        if self.keyword_metrics_client:
            # ä½¿ç”¨æ–°çš„APIæ ¼å¼
            return await self._prepare_new_api_submit_data(keyword_data, url_keywords_map)
        else:
            # ä½¿ç”¨æ—§çš„APIæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            return self._prepare_legacy_submit_data(keyword_data)

    async def _prepare_new_api_submit_data(self, keyword_data: Dict[str, Dict],
                                          url_keywords_map: Dict[str, Set[str]]) -> List[Dict]:
        """
        å‡†å¤‡æ–°APIæ ¼å¼çš„æäº¤æ•°æ®

        Args:
            keyword_data: å…³é”®è¯æ•°æ®
            url_keywords_map: URLåˆ°å…³é”®è¯é›†åˆçš„æ˜ å°„

        Returns:
            List[Dict]: ç¬¦åˆæ–°APIæ ¼å¼çš„æäº¤æ•°æ®åˆ—è¡¨
        """
        # æ„å»ºæŸ¥è¯¢APIå“åº”æ ¼å¼
        query_response = {
            'status': 'success',
            'data': []
        }

        for keyword, data in keyword_data.items():
            # æ•°æ®å·²ç»åœ¨_filter_successful_dataä¸­è¿‡æ»¤è¿‡ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
            query_response['data'].append({
                'keyword': keyword,
                'metrics': data
            })

        # ä½¿ç”¨è½¬æ¢å™¨è½¬æ¢ä¸ºæäº¤æ ¼å¼
        submit_data = self.data_transformer.transform_query_response_to_submit_format(
            query_response, url_keywords_map
        )

        # æ‰“å°è½¬æ¢ç»Ÿè®¡
        stats = self.data_transformer.get_transformation_stats(submit_data)
        self.logger.info(f"æ•°æ®è½¬æ¢å®Œæˆ: {stats}")

        return submit_data

    def _prepare_legacy_submit_data(self, keyword_data: Dict[str, Dict],
                                   url_keywords_map: Dict[str, Set[str]] = None) -> List[Dict]:
        """
        å‡†å¤‡ç¬¦åˆAPIæ–‡æ¡£è§„èŒƒçš„æäº¤æ•°æ® - ç®€åŒ–ç‰ˆæœ¬

        Args:
            keyword_data: å…³é”®è¯æ•°æ®
            url_keywords_map: URLåˆ°å…³é”®è¯çš„æ˜ å°„å…³ç³»

        Returns:
            List[Dict]: ç¬¦åˆAPIæ–‡æ¡£æ ¼å¼çš„æäº¤æ•°æ®åˆ—è¡¨
        """
        # åˆ›å»ºå…³é”®è¯åˆ°URLçš„åå‘æ˜ å°„
        keyword_to_urls = {}
        if url_keywords_map:
            for url, keywords in url_keywords_map.items():
                for keyword in keywords:
                    if keyword not in keyword_to_urls:
                        keyword_to_urls[keyword] = []
                    keyword_to_urls[keyword].append(url)

        submit_data = []

        for keyword, data in keyword_data.items():
            # åªå¤„ç†å¿…è¦çš„å­—æ®µè½¬æ¢
            # 1. nullå€¼è½¬æ¢ä¸º0
            low_bid = data.get('low_top_of_page_bid_micro', 0) or 0
            high_bid = data.get('high_top_of_page_bid_micro', 0) or 0

            # 2. ç¡®ä¿monthly_searchesä¸­çš„yearå’Œmonthä¸ºå­—ç¬¦ä¸²
            original_monthly_searches = data.get('monthly_searches', [])
            self.logger.debug(f"ğŸ” å¤„ç†å…³é”®è¯ {keyword} çš„ monthly_searches:")
            self.logger.debug(f"   åŸå§‹æ•°æ®ç±»å‹: {type(original_monthly_searches)}")
            self.logger.debug(f"   åŸå§‹æ•°æ®é•¿åº¦: {len(original_monthly_searches) if isinstance(original_monthly_searches, list) else 'N/A'}")
            self.logger.debug(f"   åŸå§‹æ•°æ®å†…å®¹: {original_monthly_searches}")

            monthly_searches = []
            for i, item in enumerate(original_monthly_searches):
                self.logger.debug(f"   å¤„ç†ç¬¬ {i+1} é¡¹: {item}")
                if isinstance(item, dict):
                    # æ”¯æŒè‹±æ–‡å’Œä¸­æ–‡å­—æ®µå
                    year_value = None
                    month_value = None
                    searches_value = None

                    # æ£€æŸ¥è‹±æ–‡å­—æ®µåï¼ˆæ­£å¸¸æƒ…å†µï¼‰
                    if 'year' in item and 'month' in item and 'searches' in item:
                        year_value = item['year']
                        month_value = item['month']
                        searches_value = item['searches']
                        self.logger.debug(f"   âœ… æ£€æµ‹åˆ°è‹±æ–‡å­—æ®µå")
                    # æ£€æŸ¥ä¸­æ–‡å­—æ®µåï¼ˆå¼‚å¸¸æƒ…å†µï¼Œéœ€è¦ä¿®å¤ï¼‰
                    elif 'å¹´' in item and 'æœˆ' in item and 'searches' in item:
                        year_value = item['å¹´']
                        month_value = item['æœˆ']
                        searches_value = item['searches']
                        # é™é»˜ä¿®å¤ä¸­æ–‡å­—æ®µåï¼Œä¸è¾“å‡ºè­¦å‘Š
                        self.logger.debug(f"   ğŸ”§ è‡ªåŠ¨ä¿®å¤ä¸­æ–‡å­—æ®µå: å¹´={year_value}, æœˆ={month_value}")

                    if year_value is not None and month_value is not None and searches_value is not None:
                        converted_item = {
                            "year": str(year_value),
                            "month": str(month_value),
                            "searches": searches_value
                        }
                        monthly_searches.append(converted_item)
                        self.logger.debug(f"   âœ… è½¬æ¢æˆåŠŸ: {converted_item}")
                    else:
                        self.logger.info(f"   âŒ è·³è¿‡æ— æ•ˆé¡¹: ç¼ºå°‘å¿…éœ€å­—æ®µ")
                else:
                    self.logger.info(f"   âŒ è·³è¿‡æ— æ•ˆé¡¹: ç±»å‹={type(item)}, ä¸æ˜¯å­—å…¸")

            self.logger.debug(f"   æœ€ç»ˆ monthly_searches é•¿åº¦: {len(monthly_searches)}")
            self.logger.debug(f"   æœ€ç»ˆ monthly_searches å†…å®¹: {monthly_searches}")

            # è·å–è¯¥å…³é”®è¯å¯¹åº”çš„URLåˆ—è¡¨
            urls = keyword_to_urls.get(keyword, [])

            # å¦‚æœæ²¡æœ‰URLæ˜ å°„ï¼Œä½¿ç”¨é»˜è®¤URL
            if not urls:
                urls = [f"https://example.com/{keyword.replace(' ', '-')}"]

            # ä¸ºæ¯ä¸ªURLåˆ›å»ºä¸€æ¡æäº¤è®°å½•
            for url in urls:
                submit_record = {
                    "keyword": keyword,
                    "url": url,  # ä½¿ç”¨çœŸå®çš„URL
                    "metrics": {
                        "avg_monthly_searches": data.get('avg_monthly_searches', 0),
                        "latest_searches": data.get('latest_searches', 0),
                        "max_monthly_searches": data.get('max_monthly_searches', 0),
                        "competition": data.get('competition', 'UNKNOWN'),
                        "competition_index": data.get('competition_index', 0),
                        "low_top_of_page_bid_micro": low_bid,
                        "high_top_of_page_bid_micro": high_bid,
                        "monthly_searches": monthly_searches,
                        "data_quality": data.get('data_quality', {
                            "status": "unknown",
                            "complete": False,
                            "has_missing_months": True,
                            "only_last_month_has_data": False,
                            "total_months": 0,
                            "available_months": 0,
                            "missing_months_count": 0,
                            "missing_months": [],
                            "warnings": ["no_data_quality_provided"]
                        })
                    }
                }

                submit_data.append(submit_record)

        return submit_data



    async def _submit_to_backend(self, submit_data: List[Dict],
                                url_keywords_map: Dict[str, Set[str]]) -> bool:
        """
        æäº¤æ•°æ®åˆ°åç«¯

        Args:
            submit_data: æäº¤æ•°æ®åˆ—è¡¨
            url_keywords_map: URLåˆ°å…³é”®è¯é›†åˆçš„æ˜ å°„

        Returns:
            bool: æ˜¯å¦æäº¤æˆåŠŸ
        """
        try:
            if self.keyword_metrics_client:
                # ä½¿ç”¨æ–°çš„å…³é”®è¯æŒ‡æ ‡å®¢æˆ·ç«¯
                success = await self.keyword_metrics_client.submit_keyword_metrics_batch(submit_data)
                if success:
                    self.logger.info(f"æˆåŠŸæäº¤ {len(submit_data)} æ¡å…³é”®è¯æŒ‡æ ‡æ•°æ®åˆ°æ–°API")
                    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœå®¢æˆ·ç«¯æ”¯æŒï¼‰
                    if hasattr(self.keyword_metrics_client, 'get_statistics'):
                        stats = self.keyword_metrics_client.get_statistics()
                        self.logger.info(f"æäº¤ç»Ÿè®¡: {stats}")
                else:
                    self.logger.error("å…³é”®è¯æŒ‡æ ‡æ•°æ®æäº¤å¤±è´¥")
                return success
            else:
                # ä½¿ç”¨åç«¯APIå®¢æˆ·ç«¯ (work.seokey.vip)
                success = await self.backend_api.submit_batch(submit_data)
                if success:
                    self.logger.info(f"æˆåŠŸæäº¤ {len(submit_data)} æ¡æ•°æ®åˆ°åç«¯API")
                else:
                    self.logger.error("æ•°æ®æäº¤å¤±è´¥")
                return success
        except Exception as e:
            self.logger.error(f"æäº¤æ•°æ®å¼‚å¸¸: {e}")
            return False
    
    def _create_empty_result(self) -> Dict[str, Any]:
        """
        åˆ›å»ºç©ºç»“æœ
        
        Returns:
            Dict[str, Any]: ç©ºç»“æœ
        """
        return {
            'total_keywords': 0,
            'successful_keywords': 0,
            'saved_urls': 0,
            'submitted_records': 0
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            'storage_stats': self.storage.get_statistics(),
            'seo_api_stats': self.seo_api.get_statistics(),
            'backend_api_stats': self.backend_api.get_statistics()
        }

    def _validate_and_convert_url_keywords_map(self, url_keywords_map) -> Dict[str, Set[str]]:
        """
        éªŒè¯å’Œè½¬æ¢url_keywords_mapæ•°æ®ç±»å‹

        Args:
            url_keywords_map: è¾“å…¥çš„URLå…³é”®è¯æ˜ å°„ï¼ˆå¯èƒ½æ˜¯å„ç§ç±»å‹ï¼‰

        Returns:
            Dict[str, Set[str]]: æ ‡å‡†åŒ–çš„URLå…³é”®è¯æ˜ å°„

        Raises:
            TypeError: å¦‚æœæ•°æ®ç±»å‹æ— æ³•è½¬æ¢
        """
        # å¦‚æœæ˜¯Noneæˆ–ç©ºï¼Œè¿”å›ç©ºå­—å…¸
        if not url_keywords_map:
            return {}

        # å¦‚æœå·²ç»æ˜¯æ­£ç¡®çš„å­—å…¸ç±»å‹ï¼Œç›´æ¥è¿”å›
        if isinstance(url_keywords_map, dict):
            # éªŒè¯å­—å…¸çš„å€¼æ˜¯å¦ä¸ºSetç±»å‹
            for url, keywords in url_keywords_map.items():
                if not isinstance(keywords, set):
                    self.logger.warning(f"URL {url} çš„å…³é”®è¯ä¸æ˜¯setç±»å‹ï¼Œæ­£åœ¨è½¬æ¢: {type(keywords)}")
                    if isinstance(keywords, (list, tuple)):
                        url_keywords_map[url] = set(keywords)
                    else:
                        self.logger.error(f"æ— æ³•è½¬æ¢å…³é”®è¯ç±»å‹: {type(keywords)}")
                        url_keywords_map[url] = set()
            return url_keywords_map

        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè¿”å›ç©ºå­—å…¸è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        if isinstance(url_keywords_map, list):
            self.logger.error(f"url_keywords_mapæ˜¯åˆ—è¡¨ç±»å‹ï¼Œæ— æ³•è½¬æ¢ä¸ºå­—å…¸: {type(url_keywords_map)}")
            self.logger.error(f"åˆ—è¡¨å†…å®¹é¢„è§ˆ: {url_keywords_map[:3] if len(url_keywords_map) > 0 else 'ç©ºåˆ—è¡¨'}")
            # è¿”å›ç©ºå­—å…¸è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ç¨‹åºç»§ç»­è¿è¡Œ
            return {}

        # å…¶ä»–ç±»å‹éƒ½æ— æ³•å¤„ç†ï¼Œè¿”å›ç©ºå­—å…¸
        self.logger.error(f"url_keywords_mapç±»å‹ä¸æ”¯æŒ: {type(url_keywords_map)}")
        return {}
    
    async def health_check(self) -> Dict[str, bool]:
        """
        å¥åº·æ£€æŸ¥
        
        Returns:
            Dict[str, bool]: å„ç»„ä»¶å¥åº·çŠ¶æ€
        """
        health_status = {}
        
        # æ£€æŸ¥åç«¯APIè¿æ¥
        try:
            health_status['backend_api'] = await self.backend_api.test_connection()
        except Exception:
            health_status['backend_api'] = False
        
        # æ£€æŸ¥SEO API
        try:
            seo_health = await self.seo_api.health_check()
            health_status['seo_api'] = any(seo_health.values())
        except Exception:
            health_status['seo_api'] = False
        
        # æ£€æŸ¥å­˜å‚¨
        health_status['storage'] = self.storage.storage_file.parent.exists()
        
        return health_status


class URLProcessor:
    """URLå¤„ç†å™¨ - è´Ÿè´£URLçš„è§£æå’Œå…³é”®è¯æå–"""
    
    def __init__(self, rule_engine, keyword_extractor):
        """
        åˆå§‹åŒ–URLå¤„ç†å™¨
        
        Args:
            rule_engine: è§„åˆ™å¼•æ“
            keyword_extractor: å…³é”®è¯æå–å™¨
        """
        self.rule_engine = rule_engine
        self.keyword_extractor = keyword_extractor
        self.logger = get_logger(__name__)
    
    def filter_processed_urls(self, urls: Set[str], storage: StorageManager) -> List[str]:
        """
        è¿‡æ»¤å·²å¤„ç†çš„URL - å¢å¼ºé”™è¯¯å¤„ç†

        Args:
            urls: URLé›†åˆ
            storage: å­˜å‚¨ç®¡ç†å™¨

        Returns:
            List[str]: æ–°URLåˆ—è¡¨
        """
        if not urls:
            self.logger.warning("è¾“å…¥URLé›†åˆä¸ºç©º")
            return []

        if not isinstance(urls, (set, list, tuple)):
            self.logger.error(f"URLsç±»å‹é”™è¯¯: {type(urls)}, æœŸæœ›set/list/tuple")
            return []

        new_urls = []
        processed_count = 0
        error_count = 0

        for url in urls:
            try:
                if not isinstance(url, str):
                    self.logger.warning(f"è·³è¿‡éå­—ç¬¦ä¸²URL: {type(url)} - {url}")
                    error_count += 1
                    continue

                if not storage.is_url_processed(url):
                    new_urls.append(url)
                else:
                    processed_count += 1

            except Exception as e:
                self.logger.error(f"æ£€æŸ¥URLå¤„ç†çŠ¶æ€å¤±è´¥ {url}: {e}")
                error_count += 1
                # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œä¿å®ˆåœ°å°†URLåŠ å…¥æ–°URLåˆ—è¡¨
                new_urls.append(url)

        self.logger.info(f"URLè¿‡æ»¤å®Œæˆ: æ–°URL {len(new_urls)}, å·²å¤„ç† {processed_count}, é”™è¯¯ {error_count}")
        return new_urls
    
    def extract_all_keywords(self, urls: List[str]) -> Dict[str, Set[str]]:
        """
        ä»æ‰€æœ‰URLæå–å…³é”®è¯ - å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬

        Args:
            urls: URLåˆ—è¡¨

        Returns:
            Dict[str, Set[str]]: URLåˆ°å…³é”®è¯é›†åˆçš„æ˜ å°„
        """
        import asyncio
        import concurrent.futures
        import os

        try:
            # å¦‚æœURLæ•°é‡è¾ƒå°‘ï¼Œä½¿ç”¨åŸå§‹é¡ºåºå¤„ç†
            if len(urls) < 1000:
                result = self._extract_keywords_sequential(urls)
            else:
                # å¹¶è¡Œå¤„ç†å¤§é‡URLï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
                result = self._extract_keywords_parallel_sync(urls)

            # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿è¿”å›å­—å…¸ç±»å‹
            if not isinstance(result, dict):
                self.logger.error(f"âŒ extract_all_keywordsè¿”å›ç±»å‹é”™è¯¯: {type(result)}, æœŸæœ›dictç±»å‹")
                return {}

            return result

        except Exception as e:
            self.logger.error(f"âŒ extract_all_keywordsæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            self.logger.error(f"âŒ å®Œæ•´å †æ ˆ: {traceback.format_exc()}")
            return {}

    def _extract_keywords_sequential(self, urls: List[str]) -> Dict[str, Set[str]]:
        """
        é¡ºåºæå–å…³é”®è¯ï¼ˆç”¨äºå°æ‰¹é‡URLï¼‰

        Args:
            urls: URLåˆ—è¡¨

        Returns:
            Dict[str, Set[str]]: URLåˆ°å…³é”®è¯é›†åˆçš„æ˜ å°„
        """
        url_keywords_map = {}
        log_interval = max(100, len(urls) // 10)
        progress = ProgressLogger(self.logger, len(urls), log_interval)

        for url in urls:
            progress.update()
            rule = self.rule_engine.get_rule_for_url(url)
            keywords = self.keyword_extractor.extract_keywords(url, rule)
            if keywords:
                url_keywords_map[url] = keywords

        progress.finish()
        self.logger.info(f"ä» {len(urls)} ä¸ªURLä¸­æå–åˆ° {len(url_keywords_map)} ä¸ªæœ‰æ•ˆURLçš„å…³é”®è¯")
        return url_keywords_map

    def _extract_keywords_parallel_sync(self, urls: List[str]) -> Dict[str, Set[str]]:
        """
        å¹¶è¡Œæå–å…³é”®è¯ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œç”¨äºå¤§æ‰¹é‡URLï¼‰

        Args:
            urls: URLåˆ—è¡¨

        Returns:
            Dict[str, Set[str]]: URLåˆ°å…³é”®è¯é›†åˆçš„æ˜ å°„
        """
        import concurrent.futures
        import os

        # ç¡®å®šå¹¶è¡Œåº¦ï¼šCPUæ ¸å¿ƒæ•°çš„2å€ï¼Œä½†ä¸è¶…è¿‡6ï¼ˆé™ä½èµ„æºæ¶ˆè€—ï¼‰
        max_workers = min(6, (os.cpu_count() or 4) * 2)

        # è®¡ç®—æ‰¹æ¬¡å¤§å°ï¼šç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡æœ‰è¶³å¤Ÿçš„å·¥ä½œé‡ï¼Œä½†ä¸ä¼šè¿‡å¤§
        batch_size = max(200, min(1000, len(urls) // (max_workers * 2)))

        self.logger.info(f"å¹¶è¡Œå…³é”®è¯æå–: {len(urls)} ä¸ªURL, {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹, æ‰¹æ¬¡å¤§å° {batch_size}")

        # åˆ†æ‰¹å¤„ç†ï¼Œé™åˆ¶å†…å­˜ä½¿ç”¨
        batches = [urls[i:i + batch_size] for i in range(0, len(urls), batch_size)]

        # è¿›åº¦è·Ÿè¸ª
        log_interval = max(500, len(urls) // 20)
        progress = ProgressLogger(self.logger, len(urls), log_interval)

        # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        url_keywords_map = {}

        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
                future_to_batch = {
                    executor.submit(self._process_url_batch, batch, progress): batch
                    for batch in batches
                }

                # æ”¶é›†ç»“æœ
                for future in concurrent.futures.as_completed(future_to_batch):
                    try:
                        batch_result = future.result()
                        url_keywords_map.update(batch_result)
                    except Exception as e:
                        batch = future_to_batch[future]
                        self.logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥ ({len(batch)} URLs): {e}")

        except Exception as e:
            self.logger.error(f"å¹¶è¡Œå¤„ç†å¼‚å¸¸: {e}")
            # å³ä½¿å‡ºç°å¼‚å¸¸ï¼Œä¹Ÿè¦å®Œæˆè¿›åº¦è®°å½•
        finally:
            progress.finish()

        self.logger.info(f"å¹¶è¡Œæå–å®Œæˆ: ä» {len(urls)} ä¸ªURLä¸­æå–åˆ° {len(url_keywords_map)} ä¸ªæœ‰æ•ˆURLçš„å…³é”®è¯")
        return url_keywords_map

    async def _extract_keywords_parallel(self, urls: List[str]) -> Dict[str, Set[str]]:
        """
        å¹¶è¡Œæå–å…³é”®è¯ï¼ˆç”¨äºå¤§æ‰¹é‡URLï¼‰

        Args:
            urls: URLåˆ—è¡¨

        Returns:
            Dict[str, Set[str]]: URLåˆ°å…³é”®è¯é›†åˆçš„æ˜ å°„
        """
        import concurrent.futures
        import os

        # ç¡®å®šå¹¶è¡Œåº¦ï¼šCPUæ ¸å¿ƒæ•°çš„2å€ï¼Œä½†ä¸è¶…è¿‡8
        max_workers = min(8, (os.cpu_count() or 4) * 2)

        # è®¡ç®—æ‰¹æ¬¡å¤§å°ï¼šç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡æœ‰è¶³å¤Ÿçš„å·¥ä½œé‡
        batch_size = max(500, len(urls) // (max_workers * 4))

        self.logger.info(f"å¹¶è¡Œå…³é”®è¯æå–: {len(urls)} ä¸ªURL, {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹, æ‰¹æ¬¡å¤§å° {batch_size}")

        # åˆ†æ‰¹å¤„ç†
        batches = [urls[i:i + batch_size] for i in range(0, len(urls), batch_size)]

        # è¿›åº¦è·Ÿè¸ª
        log_interval = max(1000, len(urls) // 20)
        progress = ProgressLogger(self.logger, len(urls), log_interval)

        # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        url_keywords_map = {}

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {
                executor.submit(self._process_url_batch, batch, progress): batch
                for batch in batches
            }

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_batch):
                try:
                    batch_result = future.result()
                    url_keywords_map.update(batch_result)
                except Exception as e:
                    batch = future_to_batch[future]
                    self.logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥ ({len(batch)} URLs): {e}")

        progress.finish()
        self.logger.info(f"å¹¶è¡Œæå–å®Œæˆ: ä» {len(urls)} ä¸ªURLä¸­æå–åˆ° {len(url_keywords_map)} ä¸ªæœ‰æ•ˆURLçš„å…³é”®è¯")
        return url_keywords_map

    def _process_url_batch(self, urls_batch: List[str], progress: 'ProgressLogger') -> Dict[str, Set[str]]:
        """
        å¤„ç†URLæ‰¹æ¬¡

        Args:
            urls_batch: URLæ‰¹æ¬¡
            progress: è¿›åº¦è®°å½•å™¨

        Returns:
            Dict[str, Set[str]]: æ‰¹æ¬¡ç»“æœ
        """
        batch_result = {}

        for url in urls_batch:
            try:
                rule = self.rule_engine.get_rule_for_url(url)
                keywords = self.keyword_extractor.extract_keywords(url, rule)
                if keywords:
                    batch_result[url] = keywords

                # çº¿ç¨‹å®‰å…¨çš„è¿›åº¦æ›´æ–°
                progress.update()

            except Exception as e:
                # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
                if self.logger.isEnabledFor(logging.DEBUG):
                    self.logger.debug(f"URLå¤„ç†å¤±è´¥ {url}: {e}")

        return batch_result
